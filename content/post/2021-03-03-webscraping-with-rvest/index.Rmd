---
title: Webscraping with rvest
author: Kiet Huynh
date: '2021-03-03'
slug: [webscraping-with-rvest]
categories:
  - R function
tags:
  - webscraping
subtitle: ''
summary: ''
authors: []
lastmod: '2021-03-03T08:04:45-08:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---

In this post, I try my hand at webscraping, a technique to retrieve data through HTML code. 

# Webscraping Example 1

I will be using the tutorial by Christian Pascual (https://www.dataquest.io/blog/web-scraping-in-r-rvest/). 
 - Will be using data on weather forecasts from the National Weather Service (https://www.weather.gov/)
 
The steps for webscraping includes the following:
- Find the HTML for the webpage to be scraped
- Identify the parts of the page you want to extract and the associated HGML/CSS
- Select the HTML

Step 1: Load rvest package
```{r}
library(rvest)
library(dplyr)
```

Step 2: Identify the parts of the page you want to extract 

-Go to website to be scraped. National Weather Service webpage for SF: https://forecast.weather.gov/MapClick.php?lat=37.777120000000025&lon=-122.41963999999996#.YD_krF1Kjzc

- Identify the specific HTML/CSS code to extract using Google Chrome's Developer and Inspection Tools (View > Developer > Developer Tools). Alternatively, you can right click highlight the click "Command + Option + C" on the part of the website you want to see the HTML/CSS code for.

- Once identified, extract that HTML text using rvest

```{r}
# Tombstone info
forecasts <- read_html("https://forecast.weather.gov/MapClick.php?lat=37.777120000000025&lon=-122.41963999999996#.YD_krF1Kjzc") %>%
    html_nodes("li.forecast-tombstone") %>%
    html_text(); forecasts

```

Note: Pascual's code did not work for me. I had to edit slightly to extract different results from the website.

# Webscraping Example 2

For this example, I will be using the following tutorial by Dmytro Perepolkin: https://rvest.tidyverse.org/articles/harvesting-the-web.html

This example will be on how to extract the names of characers the actors play in the Lego Movie on IMDB (https://www.imdb.com/title/tt1490017/).

```{r}
library(rvest)

# Read the website
lego_url <- "http://www.imdb.com/title/tt1490017"
html <- read_html(lego_url)

# Scrape list of characters
characters <- html_nodes(html, ".cast_list .character")
length(characters)
characters[1:2]

# Trim unnecssary info
html_text(characters, trim=TRUE)

# Identify name of selected tag
html_nodes(html, ".cast_list") %>%
  html_name()

# Create table from scraped data
html_node(html, ".cast_list") %>%
  html_table() %>%
  head()

# Extract text for first character in node
html_node(html, ".cast_list .character") %>%
  html_text()

# Extract attributes of HTML tags
html_nodes(html, ".cast_list .character") %>% 
  html_children() %>% 
  html_attr("href")

# Extract "children" text from "parent" text
html_nodes(html, ".cast_list .character") %>% 
  html_children() %>% 
  html_text()
```

After learning about all of the above features of rvest(), I belive the following are most useful (and I will probably use the most)

```{r}
# Read the website
lego_url <- "http://www.imdb.com/title/tt1490017"
html <- read_html(lego_url)

# Scrape list of characters
characters <- html_nodes(html, ".cast_list .character"); characters
  # Trim unnecssary info
  html_text(characters, trim=TRUE)

# Identify name of selected tag
html_nodes(html, ".cast_list") %>%
  html_name()

# Create table from scraped data
html_node(html, ".cast_list") %>%
  html_table()

# Extract "children" text from "parent" text
html_nodes(html, ".cast_list .character") %>% 
  html_children() %>% 
  html_text()
```

# Webscraping Example 3

This example is from Kieth McNulty: https://towardsdatascience.com/tidy-web-scraping-in-r-tutorial-and-resources-ac9f72b4fe47.

In this example, we try to find the Billboard Top 100 from https://www.billboard.com/charts/hot-100.

```{r}
# Load packages
library(rvest)
library(dplyr)

# Load website
hot100page <- "https://www.billboard.com/charts/hot-100"
hot100 <- read_html(hot100page)
hot100
str(hot100)

# List of nodes inside body of webpage
body_nodes <- hot100 %>% 
 html_node("body") %>% 
 html_children()
body_nodes

# See sublevels of the body nodes
body_nodes %>% 
 html_children()

# Locate the "tag" and "class" names.
# Go to the website and press "Command + Option + C" and click on the sections of the website that contains the info you want to extract (i.e., rank number, artist name, song title). When I click on song rank number, I get the following: "<span class="chart-element__rank__number">4</span>". From this, I see that the tag is called "span" and the class is called "chart-element__rank__number". Do these same steps for the artist and song name.

# Extract data/list from website using xml2() and rvest(). 
# Here we extract the rank number, artist, and song title
rank <- hot100 %>% 
  rvest::html_nodes('body') %>% 
  xml2::xml_find_all("//span[contains(@class, 'chart-element__rank__number')]") %>% 
  rvest::html_text()
artist <- hot100 %>% 
  rvest::html_nodes('body') %>% 
  xml2::xml_find_all("//span[contains(@class, 'chart-element__information__artist text')]") %>% 
  rvest::html_text()
title <- hot100 %>% 
  rvest::html_nodes('body') %>% 
  xml2::xml_find_all("//span[contains(@class, 'chart-element__information__song text')]") %>% 
  rvest::html_text()

# Compile information into table. Done!
chart_df <- data.frame(rank, artist, title); chart_df
knitr::kable(
  chart_df  %>% head(10))
```


```{r}
hot100page <- "https://www.billboard.com/charts/hot-100"
hot100 <- read_html(hot100page)
hot100
str(hot100)

body_nodes <- hot100 %>% 
 html_node("body") %>% 
 html_children()
body_nodes

body_nodes %>% 
 html_children()

rank <- hot100 %>% 
  rvest::html_nodes('body') %>% 
  xml2::xml_find_all("//span[contains(@class, 'chart-element__rank__number')]") %>% 
  rvest::html_text()
artist <- hot100 %>% 
  rvest::html_nodes('body') %>% 
  xml2::xml_find_all("//span[contains(@class, 'chart-element__information__artist')]") %>% 
  rvest::html_text()
title <- hot100 %>% 
  rvest::html_nodes('body') %>% 
  xml2::xml_find_all("//span[contains(@class, 'chart-element__information__song')]") %>% 
  rvest::html_text()

chart_df <- data.frame(rank, artist, title)
knitr::kable(
  chart_df  %>% head(10))



get_chart <- function(date = Sys.Date(), positions = c(1:10), type = "hot-100") {

  # get url from input and read html
  input <- paste0("https://www.billboard.com/charts/", type, "/", date) 
  chart_page <- xml2::read_html(input)
  
  # scrape data
  rank <- chart_page %>% 
    rvest::html_nodes('body') %>% 
    xml2::xml_find_all("//span[contains(@class, 'chart-element__rank__number')]") %>% 
    rvest::html_text()
  
  artist <- chart_page %>% 
    rvest::html_nodes('body') %>% 
    xml2::xml_find_all("//span[contains(@class, 'chart-element__information__artist')]") %>% 
    rvest::html_text()
  
  title <- chart_page %>% 
    rvest::html_nodes('body') %>% 
    xml2::xml_find_all("//span[contains(@class, 'chart-element__information__song')]") %>% 
    rvest::html_text()

  # create dataframe, remove nas and return result
  chart_df <- data.frame(rank, artist, title)
  chart_df <- chart_df %>% 
    dplyr::filter(!is.na(rank), rank %in% positions)

chart_df
}

test <- get_chart(date = "1975–01–20", positions = 1:10, type = "hot-100")
test
```


# Webscraping Example 4

https://finance.yahoo.com/quote/PS/holders?p=PS

## One website

```{r}
# --
# Importing the rvest library 
# It internally imports xml2 library too 
# --
library(rvest)


# --
# Load the link of Holders tab in a variable, here link
# --
link <- "https://finance.yahoo.com/quote/PS/holders?p=PS"


# --
# Read the HTML webpage using the xml2 package function read_html()
# --
driver <- read_html(link)


# --
# Since we know there is a tabular data on the webpage, we pass "table" as the CSS selector
# The variable "allTables" will hold all three tables in it
# --
allTables <- html_nodes(driver, css = "table")


# --
# Fetch any of the three tables based on their index
# Table 1. Major Holders
# --
majorHolders <- html_table(allTables)[[1]]
majorHolders

# --
# Table 2. Top Institutional Holders
# --
topInstHolders <- html_table(allTables)[[2]]
topInstHolders

# --
# Table 3. Top Mutual Fund Holders
# --
topMutualFundHolders <- html_table(allTables)[[3]]
topMutualFundHolders
```

## Multiple website

This r code scrapes info about classess provided by pluralsight.com. The classes are listed on 10 different webpages linked to the homepage. The r code will automatically go to each webpage and scrape the necessary information and then compile it into a table.

```{r}
library(rvest)
library(stringr) # For data cleaning

link <- "https://www.pluralsight.com/browse"

driver <- read_html(link)

# Extracting sub URLs
# Here, tile-box is a parent class which holds the content in the nested class.
# First, go inside the sub-class using html_children() and then fetch the URLs to each Skill page
subURLs <- html_nodes(driver,'div.tile-box') %>% 
            html_children() %>% 
            html_attr('href')

# Removing NA values and last `/browse` URL
subURLs <- subURLs[!is.na(subURLs)][1:10]

# Main URL - to complete the above URLs
mainURL <- "https://www.pluralsight.com"

# This function fetches those four entities as you learned in the previous section of this guide
entity <- function(s){
  
  # Course Title
  # Since Number of Courses may differ from Skill to Skill, therefore,
  # we have done dynamic fetching of the course names
  
  v <- html_nodes(s, "div.course-item__info") %>%
    html_children() 
  
  titles <- gsub("<|>", "", str_extract(v[!is.na(str_match(v, "course-item__title"))], ">.*<"))
  
  # Course Authors
  authors <- html_nodes(s, "div.course--item__list.course-item__author") %>% html_text()
  
  # Course Level
  level <- html_nodes(s, "div.course--item__list.course-item__level") %>% html_text()
  
  # Course Duration
  duration <- html_nodes(s, "div.course--item__list.course-item__duration") %>% html_text()
  
  # Creating a final DataFrame
  courses <- data.frame(titles, authors, level, duration)
  
  return(courses)
}


# A for loop which goes through all the URLs, fetch the entities and display them on the screen 
i = 1
for (i in 1:10) {
  subDriver <- read_html(paste(mainURL, subURLs[i], sep = ""))
  print(entity(subDriver))
}
```


# [ ] Wescrapting Example 5

https://www.datacamp.com/community/tutorials/r-web-scraping-rvest


























